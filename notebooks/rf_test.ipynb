{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['retweet'] = df.apply(lambda row: count_retweets(row['tweet']),axis=1)\n",
    "    df2['mentions'] = df.apply(lambda row : count_mentions(row['tweet']),axis=1)\n",
    "    df2['urls'] = df.apply(lambda row : count_urls(row['tweet']),axis=1)\n",
    "    df2['hashtags'] = df.apply(lambda row : count_hashtags(row['tweet']),axis = 1)\n",
    "    df['preprocess'] = df.apply(lambda row: remove_retweets(row['tweet']), axis=1)\n",
    "    df['preprocess'] = df.apply(lambda row: remove_mentions(row['preprocess']), axis=1)\n",
    "    df['preprocess'] = df.apply(lambda row: remove_urls(row['preprocess']), axis=1)\n",
    "    df['preprocess'] = df.apply(lambda row: remove_hashtags(row['preprocess']), axis=1)\n",
    "    df['preprocess'] = df.apply(lambda row: remove_additional_space(row['preprocess']), axis=1)\n",
    "    df['preprocess'] = df.apply(lambda row: replace_slash_chars_by_space(row['preprocess']), axis=1)\n",
    "    df['preprocess'] = df.apply(lambda row: remove_underscore(row['preprocess']), axis=1)\n",
    "    df['preprocess'] = df.apply(lambda row: remove_emojis(row['preprocess']), axis=1)\n",
    "    with open('../data/english_stopwords.txt', 'r') as f:\n",
    "        stopwords = f.read().split('\\n')\n",
    "    df['preprocess'] = df.apply(lambda row: row['preprocess'].lower(), axis=1)\n",
    "    to_remove_stopwords = [\"not\", \"no\", \"wouldn't\", \"shouldn't\", \"couldn't\", \"won't\", \"can't\", \"doesn't\", \"isn't\", \"wasn't\", \"don't\"]\n",
    "    final_stopwords = [s for s in stopwords if s not in to_remove_stopwords]\n",
    "    df['preprocess'] = df.apply(lambda row: remove_stopwords(row['preprocess'], final_stopwords), axis=1)\n",
    "    \n",
    "    return df,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index                                              tweet  label  \\\n",
      "31996  31996  https://t.co/inx3HTRcfR Smell Diminishes by Da...      1   \n",
      "31997  31997  COVID-19 outbreak on the Diamond Princess crui...      1   \n",
      "31998  31998  Coronavirus China Italy | Coronavirus Outbreak...      1   \n",
      "31999  31999  Coronavirus Daily Digest: April 13, 2020\\nhttp...      1   \n",
      "32000  32000  Detention centers are notorious for having poo...      1   \n",
      "\n",
      "                                              preprocess  \n",
      "31996        smell diminishes day 3 covid-19, study says  \n",
      "31997  covid-19 outbreak diamond princess cruise ship...  \n",
      "31998  coronavirus china italy | coronavirus outbreak...  \n",
      "31999           coronavirus daily digest: april 13, 2020  \n",
      "32000  detention centers notorious poor conditions &a...  \n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "database = \"final_merged_dataset.csv\"\n",
    "df = pd.read_csv(data_path + database)\n",
    "df['Unnamed: 0'] = [i for i in range(len(df))] #c'est moche mais ca marche c'est la vie \n",
    "df.columns = ['index', 'tweet', 'label']\n",
    "\n",
    "df,df2 = preprocess(df)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "test = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = df['preprocess'] # use 'preprocess' for preprocessed tweets, 'tweet' for unpreprocessed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 100, analyzer = stemmed_words)\n",
    "\n",
    "vectorizer.fit(corpus)\n",
    "X = vectorizer.transform(corpus).toarray()\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit(X)\n",
    "X = transformer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.55609034, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.13991225, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.27554131],\n",
       "       [0.        , 0.91392295, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.concatenate((X,test),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "svm = SVC(kernel = 'linear')\n",
    "nb = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "ada = AdaBoostClassifier(n_estimators = 200)\n",
    "classifiers = [rf,svm,nb,lr,ada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle RandomForestClassifier(n_estimators=50) : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88      3499\n",
      "           1       0.92      0.90      0.91      4502\n",
      "\n",
      "    accuracy                           0.90      8001\n",
      "   macro avg       0.90      0.90      0.90      8001\n",
      "weighted avg       0.90      0.90      0.90      8001\n",
      "\n",
      "Modèle SVC(kernel='linear') : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      3335\n",
      "           1       0.87      0.82      0.85      4666\n",
      "\n",
      "    accuracy                           0.83      8001\n",
      "   macro avg       0.82      0.83      0.82      8001\n",
      "weighted avg       0.83      0.83      0.83      8001\n",
      "\n",
      "Modèle GaussianNB() : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74      3268\n",
      "           1       0.84      0.78      0.81      4733\n",
      "\n",
      "    accuracy                           0.78      8001\n",
      "   macro avg       0.77      0.78      0.78      8001\n",
      "weighted avg       0.79      0.78      0.78      8001\n",
      "\n",
      "Modèle LogisticRegression() : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      3321\n",
      "           1       0.87      0.82      0.85      4680\n",
      "\n",
      "    accuracy                           0.82      8001\n",
      "   macro avg       0.82      0.83      0.82      8001\n",
      "weighted avg       0.83      0.82      0.83      8001\n",
      "\n",
      "Modèle AdaBoostClassifier(n_estimators=200) : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83      3415\n",
      "           1       0.88      0.84      0.86      4586\n",
      "\n",
      "    accuracy                           0.85      8001\n",
      "   macro avg       0.84      0.85      0.84      8001\n",
      "weighted avg       0.85      0.85      0.85      8001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0)\n",
    "for clf in classifiers :\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f'Modèle {clf} : \\n' + classification_report(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf2 = RandomForestClassifier()\n",
    "ada2 = AdaBoostClassifier()\n",
    "params = {'n_estimators' : [20,50,100,150,200]}\n",
    "clf1 = GridSearchCV(rf2,params)\n",
    "clf2 = GridSearchCV(ada2,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = clf1.fit(X_train,Y_train)\n",
    "ada_results = clf2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3089  527]\n",
      " [ 397 3988]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      location                                               text  value  \\\n",
      "0      22894.0  COVID and Sleep: Sweet Dreams Aren’t Made of T...    1.0   \n",
      "1      18071.0  Seriously?\\n\\nWalmart to Shut Down 269 Stores,...    0.0   \n",
      "2      30784.0  Is your local government doing enough testing ...    1.0   \n",
      "3      30695.0  @ChuckCallesto Wearing a cloth face cover will...    1.0   \n",
      "4      26966.0  I've been saying this since February: it's AIR...    1.0   \n",
      "...        ...                                                ...    ...   \n",
      "1250   11589.0  The positive rate of these tests is dropping a...    1.0   \n",
      "1251   14358.0  Delhi Public School Selling Face Masks For Rs....    0.0   \n",
      "1252   20861.0  Vitamin C protects against Coronavirus - make ...    0.0   \n",
      "1253    1191.0  Israeli Researchers Near Covid-19 Vaccine Deve...    0.0   \n",
      "1254    4763.0  Govt has clarified that Pradhan Mantri Suraksh...    0.0   \n",
      "\n",
      "      predicted                                         preprocess  \n",
      "0           0.0              covid sleep: sweet dreams aren’t made  \n",
      "1           1.0  seriously? walmart shut 269 stores, including ...  \n",
      "2           0.0  local government enough testing mitigate suppr...  \n",
      "3           0.0  wearing cloth face cover not prevent virus par...  \n",
      "4           0.0  i've saying since february: airborn wear mask ...  \n",
      "...         ...                                                ...  \n",
      "1250        0.0  positive rate tests dropping least little. ove...  \n",
      "1251        1.0     delhi public school selling face masks rs. 400  \n",
      "1252        1.0  vitamin c protects coronavirus - make - cheap ...  \n",
      "1253        1.0  israeli researchers near covid-19 vaccine deve...  \n",
      "1254        1.0  govt clarified pradhan mantri suraksha bima yo...  \n",
      "\n",
      "[1255 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "errors = pd.DataFrame({'location': [], 'text' :[], 'value': [], 'predicted': []})\n",
    "for i in range(len(Y_pred)):\n",
    "    if Y_test.values[i] - Y_pred[i] != 0:\n",
    "        errors = errors.append({\n",
    "            'location': Y_test.index[i],\n",
    "            'text': df['tweet'][Y_test.index[i]],\n",
    "            'preprocess': df['preprocess'][Y_test.index[i]],\n",
    "            'predicted': Y_pred[i],\n",
    "            'value': Y_test.values[i]\n",
    "        }, ignore_index= True)\n",
    " \n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample error : \n",
      " COVID and Sleep: Sweet Dreams Aren’t Made of This https://t.co/wb381F0JAi \n",
      "\n",
      "Preprocessed error : \n",
      " covid sleep: sweet dreams aren’t made \n",
      "\n",
      "Line :  22894.0\n",
      "Predicted value :  0.0 \n",
      "Real value :  1.0 \n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Sample error : \n",
      " Seriously?\n",
      "\n",
      "Walmart to Shut Down 269 Stores, Including 154 in US - Red Lake Nation News https://t.co/yrimsFso6m \n",
      "\n",
      "Preprocessed error : \n",
      " seriously? walmart shut 269 stores, including 154 us - red lake nation news \n",
      "\n",
      "Line :  18071.0\n",
      "Predicted value :  1.0 \n",
      "Real value :  0.0 \n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Sample error : \n",
      " Is your local government doing enough testing to mitigate and suppress COVID-19?\n",
      "\n",
      "The #TTSICollaborative launches its interactive COVID Risk Map. See the most up-to-date data and read our policy recommendations and metrics for success.\n",
      "https://t.co/bNny3W7bQt https://t.co/t6ttcc2UKI \n",
      "\n",
      "Preprocessed error : \n",
      " local government enough testing mitigate suppress covid-19? ttsicollaborative launches interactive covid risk map. see up-to-date data read policy recommendations metrics success. \n",
      "\n",
      "Line :  30784.0\n",
      "Predicted value :  0.0 \n",
      "Real value :  1.0 \n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Sample error : \n",
      " @ChuckCallesto Wearing a cloth face cover will not prevent virus particles from getting through. I think they should be optional. If you are sick stay home, unfortunately 60% or more are asymtomatic when infected with the coronavirus. \n",
      "\n",
      "Preprocessed error : \n",
      " wearing cloth face cover not prevent virus particles getting through. think optional. sick stay home, unfortunately 60% asymtomatic infected coronavirus. \n",
      "\n",
      "Line :  30695.0\n",
      "Predicted value :  0.0 \n",
      "Real value :  1.0 \n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Sample error : \n",
      " I've been saying this since February: it's AIRBORN\n",
      "WEAR A MASK😷😷\n",
      "\n",
      "COVID-19 Is Likely Airborne, Aerosol Scientist Says https://t.co/eHiT8Ac2OE #SmartNews \n",
      "\n",
      "Preprocessed error : \n",
      " i've saying since february: airborn wear mask covid-19 likely airborne, aerosol scientist says smartnews \n",
      "\n",
      "Line :  26966.0\n",
      "Predicted value :  0.0 \n",
      "Real value :  1.0 \n",
      "\n",
      "_______________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for error in errors.head().iterrows():\n",
    "    print('Sample error : \\n', error[1]['text'], '\\n')\n",
    "    print('Preprocessed error : \\n', error[1]['preprocess'], '\\n')\n",
    "    print('Line : ', error[1]['location'])\n",
    "    print('Predicted value : ', error[1]['predicted'] , '\\nReal value : ', error[1]['value'], '\\n')\n",
    "    print('_______________________________________________\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
